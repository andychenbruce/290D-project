\documentclass{article}
\usepackage[preprint]{neurips_2023}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\title{Approximating Programs as Neural Networks for Automated Correction}

\author{
  Andrew Bruce \\ \href{mailto:acbruce@ucsc.edu}{acbruce@ucsc.edu}
  \and
  Dongjing Wang \\ \href{mailto:dwang114@ucsc.edu}{dwang114@ucsc.edu}
  \and
  Ming Qi \\ \href{mailto:mqi6@ucsc.edu}{mqi6@ucsc.edu}
}

\begin{document}

\maketitle

\begin{abstract}
  We present an approach for automated program correction given test cases. Our framework approaches discreet operations as a stoichastic process, representing scalars as gaussian distributions. Each distribution has a mean of the value of the scalar, with the variance as a hyperparameter. This representation allows differentiation of usually discontinious operations, and applying machine learning gradient descent on the program with respect to parameters. In this paper we show training methods, hyperparameter estimation, and results at correction. These methods could potentially have applications in increasing accuraccy of LLM code generation and hash function attacks.
\end{abstract}

\section*{Motivation}
Debugging and correcting software errors can be both time-consuming and error-prone. Traditional automated repair tools rely on static analysis, fuzzing, and constraint solving. We aimed to create a system which fits a codebase to a set of user defined behaviour.
\section*{Methods}
If we can apply a softening transformation (ST) to discrete operations in code (conditionals, integer operations, list indexing indexing) into a differential approximation, then the program itself can be turned into a neural network. We do this with a stoichastic interpretation. For training, we take a set of test cases or specifications that define correctness conditions, we can backpropagate from a loss function representing deviation from correct behavior. We hope that machine learning methods can correct errors.
\subsection*{Stoichastic perspective}
The model is defined by 3 hyperparameters along with program AST $T$, each of them is a variance for a stoichastic operation, comparisons, equality, and list indexing as $\sigma_{\text{cmp}}$, $\sigma_{\text{eq}}$, and $\sigma_{\text{list}}$ respectivley.
\begin{center}
  $\sigma_{\text{cmp}} \in \mathbb{R}$\\
  $\sigma_{\text{eq}} \in \mathbb{R}$\\
  $\sigma_{\text{list}} \in \mathbb{R}$
\end{center}

Cumulative comparison operations can be calculated with a convolution over the probability distributions of 2 scalars. With 2 scalars $a$ and $b$ with variances $\sigma_a$ and $\sigma_b$ we can defined softening transformations. With $\Psi$ as the Gaussian CDF, the less than operator ST can be defined as below.
\begin{center}
  HI: $a < b \in \{\text{true}, \text{false}\}$\\
  SI: $P(a - b < 0) = \int_{-\infty}^0 (a \star b) (k) dt = \int_{-\infty}^0 \int_{-\infty}^{\infty} \dfrac{1}{\sigma_a \sqrt{\tau}} \dfrac{1}{\sigma_b \sqrt{\tau}} e^{(\frac{k - a}{\sigma_a})^2}e^{(\frac{t-k + b}{\sigma_b})^2} dk dt = \int_{-\infty}^0 \dfrac{1}{\sqrt{\sigma_a^2 + \sigma_b^2} \sqrt{\tau}} e^{\frac{(t - a + b)^2}{\sigma_a^2 + \sigma_b^2}}dt = \Psi(a - b, \sqrt{\sigma_a^2 + \sigma_b^2}) \in [0, 1]$
\end{center}
The Gaussian CDF can then be approximated with a sigmoid function which is easier to calculate. The variance is defined as the model hyperparameter $\sigma_{\text{cmp}}$.
\begin{center}
  $\Psi(a - b, \sqrt{\sigma_a^2 + \sigma_b^2}) \approx \sigma(a - b, \sqrt{\sigma_a^2 + \sigma_b^2}) = \sigma(a - b, \sigma_{\text{cmp}}) \in [0, 1]$
\end{center}

For equality we use the inner product as a similarity function, and then normalize to keep the values between 0 and 1 for booleans.
\begin{center}
  
\end{center}



\subsection*{Hyperparameter optimizations}
As the hyperparameter variances approach zero, the approximation should approach the real program. $\lim_{\sigma_{\text{cmp}}, \sigma_{\text{eq}}, \sigma_{\text{list}} \rightarrow 0} SI(T) = HI(T)$.

To deal with problems such as operatrs not scaling with program constant sizes we start with the hyperparameter variances, high and decrease them towards zero each epoch. This approach towards zero is done with an exponential decay.\\

For an epoch $i$, we found the below hyperparameter variance decay to train the model well. 
\begin{center}
  $\sigma_{\text{cmp}} = \text{max}(e^{-\frac{i}{300}}, 0.0001)$\\
  $\sigma_{\text{eq}} = \text{max}(20000e^{-\frac{i}{300}}, 0.0001)$\\
  $\sigma_{\text{cmp}} = \text{max}(e^{-\frac{i}{300}}, 0.0001)$\\
\end{center}
Due to floating point precisoin, a lower bound clamping is necessary to avoid zero derivatives and division by zeros at higher gradients. This decay set is so that at higher epochs the SI's behaviour more closeley resembles that as the HI, but at low epochs still allows the derivatives of far off comparisons, equalities, and indexes to have non-neglegable derivatives for practical training. If the variances do reach zero, gaussians will become delta functions, and sigmoids become step functions, recovering the HI behaviour [ADD REFERENCE HERE], though this is impractical with real floating point implementation.

\section*{Stuff}

We build upon theoretical insights from differentiable programming \cite{blondel2024elementsdifferentiableprogramming, DBLP:journals/corr/abs-1907-07587, vandemeulebroucke2018myia} and related works. Our contributions include:
\begin{itemize}
\item A type system and automatic differentiation framework that transforms discrete imperative code into a differentiable analog.
\item A hard interpreter (HI) for original code and a soft interpreter (SI) for its differentiable transformation.
    \item Approaches to complex features: dynamic loss function generation from test cases, handling while loops as Markov chains, differentiable dictionaries, and attempted explorations of Hessian-based optimization methods.
    \item Hyper parameter optimizations: we tested methods such as the real time adjustment of model parameters during the training process, and adjusting hyperparameters based on the 
    \item An evaluation showing the feasibility of correcting simple program errors through gradient-based updates.
\end{itemize}

\section{Background and Related Work}
\paragraph{Differentiable Programming.} Differentiable programming aims to integrate gradient-based optimization into the software development process, allowing parameters within code to be tuned automatically \cite{blondel2024elementsdifferentiableprogramming,DBLP:journals/corr/abs-1907-07587,vandemeulebroucke2018myia}. Earlier works have explored differentiable interpreters and soft approximations of discrete structures, but significant challenges remain in applying these ideas to general imperative programs.

\paragraph{Automated Program Repair.} Existing automated repair techniques often rely on search strategies or constraint-based methods. These approaches lack a direct gradient signal and often struggle to handle large search spaces efficiently. Our method proposes leveraging gradient-based optimization to guide corrections, potentially converging faster in parameterized programs.

\section{Methodology}

\subsection{Auto-Differentiation Framework and Custom Type System}
To apply deep learning techniques, we must convert programs into a form amenable to backpropagation. We implement an auto-differentiation framework that operates on an intermediate representation of the code, modeled as a computational graph. This involves:
\begin{itemize}
    \item \textbf{Custom Type System:} Variables and values in the code are assigned types that describe how they interact and can be ``softened.'' For instance, booleans become probabilities in $[0,1]$, integers become real numbers, and indexing operations become probabilistic lookups.
    \item \textbf{Soft Interpreter:} Alongside the original (hard) interpreter that executes discrete code, we define a soft interpreter that executes the softened code. Control flow, arithmetic, and data structure operations are replaced with smooth approximations.
    \item \textbf{Gradient Computation:} We manually implement both first- and second-order derivatives (gradients and Hessians) to enable backpropagation and potential use of second-order optimization methods.
\end{itemize}

\subsection{Softening and Hardening Code}
The core idea is to replace discrete program constructs with differentiable counterparts:
\begin{itemize}
    \item \textbf{Boolean Softening:} A boolean condition is replaced by a continuous probability (e.g., via a sigmoid function). Thus, `if (x > 2)` becomes `if (sigmoid(x - 2))`.
    \item \textbf{Integer Softening:} Integers can be represented by real numbers. For indexing, we may represent indices as Gaussian distributions over array positions, enabling differentiable approximations of discrete indexing.
\end{itemize}

After optimization converges, a ``hardening'' step maps these differentiable approximations back to discrete values. For booleans, we choose the nearest discrete value (e.g., thresholding at 0.5). For integers, we round to the nearest integer. This final hardening step returns the modified program to a standard executable form.

\subsection{Dynamic Loss Function Generation}
The loss function is derived from user-provided test cases that specify correct program behavior. Each test case provides input-output pairs, and the difference between the program's soft output and the expected output defines a loss. We aggregate losses over all test cases, and the optimization process attempts to minimize this aggregated loss.

As the model trains, continuous values that represent conditions or parameters in the code adjust to reduce loss. When the loss approaches zero, the softened approximation aligns closely with the target specification. However, due to approximation and non-smoothness, zero loss does not guarantee perfect discrete correctness.

\section{Advanced Techniques}

\subsection{Hessian Methods}
While gradient-based optimizers (e.g., SGD, Adam) are $O(n)$ in the number of parameters, exploring second-order methods like Newton's method can improve convergence. Computing Hessians is $O(n^2)$ and requires complicated analysis, but may speed convergence for small programs. We experimented with computing Hessians directly, though non-differentiability and scaling issues remain challenging.

\subsection{Markovian While Loops}
While loops are problematic because their iteration count is not fixed and can be infinite. We approximate while loops as Markov chains, with transition probabilities representing the likelihood of continuing the loop. By limiting the depth of unrolling or relying on probabilistic approximations, we can differentiate through loop constructs. Although infinite loops cannot be fully captured, this approach allows partial differentiability for loops.

\subsection{Differentiable Dictionaries}
Key-value lookups in dictionaries are inherently discrete. We implement dictionaries as kernel-based lookups over continuous key embeddings. Inspired by self-attention mechanisms, we represent keys as vectors and compute a weighted sum of values based on similarity. This enables differentiability in dictionary operations, albeit with increased computational overhead.

\subsection{Synthetic Data Generation}
To train and evaluate the framework, we generate synthetic data by perturbing abstract syntax trees (ASTs) of correct programs. This produces incorrect variants and their corresponding test cases. This approach can generate large training sets, facilitating stable training and evaluation of the method’s ability to correct errors.

\section{Results and Discussion}
We implemented our framework and tested it on simple toy programs. For instance, consider a program that checks if a number is greater than two but is incorrectly using the threshold of four. By softening the comparison and introducing a parameter $C$, we can backpropagate from test cases (e.g., inputs around 2) to adjust $C$ towards the correct threshold.

Empirically, we observed:
\begin{itemize}
    \item Programs with simple arithmetic and conditional logic can be partially corrected by gradient-based updates.
    \item Loss convergence suggests that softened parameters often move toward correct values.
    \item More complex programs with loops, dictionaries, and intricate logic remain challenging due to non-differentiability and high computational costs.
\end{itemize}

\section{Limitations}
Several obstacles remain:
\begin{itemize}
    \item \textbf{Hessian Invertability:} ended up giving up on this
    \item \textbf{While loops are slow:} Infinite loops, variable-length lists, and string keys are difficult to soften. Many data structures must be approximated in restrictive ways.
    \item \textbf{Scalability:} Gradient computations, especially Hessians, grow quickly in cost. Larger programs may be infeasible to handle with this approach.
\end{itemize}

\section{Conclusion and Future Work}
This work demonstrates a proof-of-concept for automatically correcting program errors using deep learning optimization methods. By approximating programs as neural networks and defining differentiable substitutes for discrete operations, we open the door to gradient-based program repair.

Future directions include:
\begin{itemize}
    \item \textbf{Richer Data Structures and Control Flow:} Expanding differentiability to handle more complex language features efficiently.
    \item \textbf{Performance Optimization:} Leveraging GPU acceleration and more efficient differentiation frameworks to scale to larger programs.
    \item \textbf{Advanced Optimization Methods:} Improving convergence with second-order methods and more sophisticated meta-optimizers.
    \item \textbf{Comprehensive Evaluation:} Testing on non-trivial codebases and real-world bugs to assess practical utility.
\end{itemize}

Though many challenges remain, our approach represents a step toward integrating neural optimization into software development workflows, potentially reducing debugging effort and improving program reliability.

\bibliographystyle{abbrv}
\bibliography{refs}


\end{document}


\documentclass{article}
